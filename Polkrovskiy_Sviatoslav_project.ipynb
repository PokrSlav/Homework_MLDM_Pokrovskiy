{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63b3c40f",
   "metadata": {},
   "source": [
    "# Project Pokrovskiy Sviatoslav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a727746",
   "metadata": {},
   "source": [
    "## Paper: BinaryConnect: Training Deep Neural Networks with binary weights during propagations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8609b35d",
   "metadata": {},
   "source": [
    "The main point of the selected article that all my privious expirience using NN was finished at the moment of training then my computer could not make necessary calculations. I keen on studing varios way to using the deep leaning metods on not such power devices.\n",
    "In this part of project i try to understand the approach of BinaryConnect and use it in fasion MNIST dataset.\n",
    "The authors use library PyLearn2 wich doesn't work on python3 and higher. It was a great problem for me, but i use their code for understading using of BinaryConnect.\n",
    "Here i use pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320ec3c9",
   "metadata": {},
   "source": [
    "### Customization the layers and conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70945c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Module, Conv2d, Linear\n",
    "from torch.nn.functional import linear, conv2d\n",
    "\n",
    "def Binarize(tensor,quant_mode='det'):\n",
    "    if quant_mode=='det':\n",
    "        return tensor.sign()\n",
    "    if quant_mode=='bin':\n",
    "        return (tensor>=0).type(type(tensor))*2-1\n",
    "    else:\n",
    "        return tensor.add_(1).div_(2).add_(torch.rand(tensor.size()).add(-0.5)).clamp_(0,1).round().mul_(2).add_(-1)\n",
    "\n",
    "\n",
    "class BNNLinear(Linear):\n",
    "\n",
    "    def __init__(self, *kargs, **kwargs):\n",
    "        super(BNNLinear, self).__init__(*kargs, **kwargs)\n",
    "        self.register_buffer('weight_org', self.weight.data.clone())\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        if (input.size(1) != 784) and (input.size(1) != 3072):\n",
    "            input.data=Binarize(input.data)\n",
    "            \n",
    "        self.weight.data=Binarize(self.weight_org)\n",
    "        out = linear(input, self.weight)\n",
    "\n",
    "        if not self.bias is None:\n",
    "            self.bias.org=self.bias.data.clone()\n",
    "            out += self.bias.view(1, -1).expand_as(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "class BNNConv2d(Conv2d):\n",
    "\n",
    "    def __init__(self, *kargs, **kwargs):\n",
    "        super(BNNConv2d, self).__init__(*kargs, **kwargs)\n",
    "        self.register_buffer('weight_org', self.weight.data.clone())\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.size(1) != 3:\n",
    "            input.data = Binarize(input.data)\n",
    "        \n",
    "        self.weight.data=Binarize(self.weight_org)\n",
    "        \n",
    "\n",
    "        out = conv2d(input, self.weight, None, self.stride,\n",
    "                                   self.padding, self.dilation, self.groups)\n",
    "\n",
    "        if not self.bias is None:\n",
    "            self.bias.org=self.bias.data.clone()\n",
    "            out += self.bias.view(1, -1, 1, 1).expand_as(out)\n",
    "\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f566a549",
   "metadata": {},
   "source": [
    "### Customization the model of BNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6bf653cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BNNCaffenet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(BNNCaffenet, self).__init__()\n",
    " \n",
    "        self.features = nn.Sequential(\n",
    "                \n",
    "                BNNConv2d(1, 96, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(96),\n",
    "                nn.Hardtanh(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=True),\n",
    "                \n",
    "                BNNConv2d(96, 192, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(192),\n",
    "                nn.Hardtanh(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=True),\n",
    "                \n",
    "                BNNConv2d(192, 288, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(288),\n",
    "                nn.Hardtanh(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=True),\n",
    "                \n",
    "                nn.Flatten(),\n",
    "                nn.BatchNorm1d(4608),\n",
    "                nn.Hardtanh(inplace=True),\n",
    "                BNNLinear(4608, num_classes),\n",
    "                nn.BatchNorm1d(num_classes, affine=False),\n",
    "                nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "\n",
    "\n",
    "    def init_w(self):\n",
    "        # weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "        return\n",
    "\n",
    "\n",
    "def bnn_caffenet(num_classes=10):\n",
    "    return BNNCaffenet(num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fc7b87",
   "metadata": {},
   "source": [
    "### Customization the classifier of the model and saving the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62ea8786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torch import save, no_grad\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "\n",
    "class BnnClassifier():\n",
    "    def __init__(self, model, train_loader=None, test_loader=None, device=None):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def save_checkpoint(state, is_best, checkpoint):\n",
    "        head, tail = os.path.split(checkpoint)\n",
    "        if not os.path.exists(head):\n",
    "            os.makedirs(head)\n",
    "\n",
    "        filename = os.path.join(head, '{0}_checkpoint.pth.tar'.format(tail))\n",
    "        save(state, filename)\n",
    "        if is_best:\n",
    "            shutil.copyfile(filename, os.path.join(head,\n",
    "                '{0}_best.pth.tar'.format(tail)))\n",
    "\n",
    "        return\n",
    "\n",
    "    def test(self, criterion):\n",
    "        self.model.eval()\n",
    "        top1 = 0\n",
    "        test_loss = 0.\n",
    "\n",
    "        with no_grad():\n",
    "            for data, target in tqdm(self.test_loader):\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                output = self.model(data)\n",
    "                test_loss += criterion(output, target).item()\n",
    "                pred = output.argmax(dim=1, keepdim=True)      \n",
    "                top1 += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        top1_acc = 100. * top1 / len(self.test_loader.sampler)\n",
    "\n",
    "        return top1_acc\n",
    "\n",
    "\n",
    "    def top1_accuracy(self):\n",
    "        return top1_accuracy(self.model, self.test_loader, self.device)\n",
    "\n",
    "\n",
    "    def train_step(self, criterion, optimizer):\n",
    "        losses = []\n",
    "        for data, target in tqdm(self.train_loader,\n",
    "                total=len(self.train_loader)):\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            output = self.model(data)\n",
    "            loss = criterion(output, target)\n",
    "            losses.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            for p in self.model.modules():\n",
    "                if hasattr(p, 'weight_org'):\n",
    "                    p.weight.data.copy_(p.weight_org)\n",
    "            optimizer.step()\n",
    "            for p in self.model.modules():\n",
    "                if hasattr(p, 'weight_org'):\n",
    "                    p.weight_org.data.copy_(p.weight.data.clamp_(-1,1))\n",
    "        return losses\n",
    "\n",
    "    def train(self, criterion, optimizer, epochs, scheduler,\n",
    "            checkpoint=None):\n",
    "\n",
    "        if checkpoint is None:\n",
    "            raise ValueError('Specify a valid checkpoint')\n",
    "\n",
    "        \n",
    "        best_accuracy = 0.\n",
    "\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "\n",
    "\n",
    "\n",
    "        for epoch in range(1, epochs+1):\n",
    "            self.model.train()\n",
    "            epoch_losses = self.train_step(criterion, optimizer)\n",
    "            losses += epoch_losses\n",
    "            epoch_losses = np.array(epoch_losses)\n",
    "            lr = optimizer.param_groups[0]['lr']  \n",
    "            test_accuracy = self.test(criterion)\n",
    "            accuracies.append(test_accuracy)\n",
    "            if scheduler:     \n",
    "                scheduler.step()\n",
    "            is_best = test_accuracy > best_accuracy\n",
    "            if is_best:\n",
    "                best_accuracy = test_accuracy\n",
    "            \n",
    "            print('Train Epoch {0}\\t Loss: {1:.6f}\\t Test Accuracy {2:.3f} \\t lr: {3:.4f}'\n",
    "                    .format(epoch, epoch_losses.mean(), test_accuracy, lr))\n",
    "            print('Best accuracy: {:.3f} '.format(best_accuracy))\n",
    "\n",
    "            self.save_checkpoint({\n",
    "                'epoch': epoch+1,\n",
    "                'state_dict': self.model.state_dict(),\n",
    "                'best_accuracy': best_accuracy,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'criterion': criterion,\n",
    "                }, is_best, checkpoint)\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f2b51d",
   "metadata": {},
   "source": [
    "### Configurable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "98809794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR = 0.001\n",
      "Steps = [80, 200]\n",
      "Gamma = 0.1\n",
      "num_epochs = 15\n",
      "Checkpoint = results/bnn_caffenet_fasion_MNIST\n"
     ]
    }
   ],
   "source": [
    "# LR \n",
    "lr = .001\n",
    "print(\"LR = \"+str(lr))\n",
    "\n",
    "# Steps\n",
    "steps = [80,200]\n",
    "print(\"Steps = \"+str(steps))\n",
    "\n",
    "# Gamma\n",
    "gamma = 0.1\n",
    "print(\"Gamma = \"+str(gamma))\n",
    "\n",
    "# Num_epochs\n",
    "epochs = 15\n",
    "print(\"num_epochs = \"+str(epochs))\n",
    "\n",
    "# Checkpoint\n",
    "checkpoint = \"results/bnn_caffenet_fasion_MNIST\"\n",
    "print(\"Checkpoint = \"+str(checkpoint))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbef600",
   "metadata": {},
   "source": [
    "### Realization of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b6c95988",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download ok\n",
      "data ok\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 1e-05\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 600/600 [05:51<00:00,  1.71it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:09<00:00, 10.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 1\t Loss: 1.361141\t Test Accuracy 67.190 \t lr: 0.0010\n",
      "Best accuracy: 67.190 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 600/600 [05:28<00:00,  1.83it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:09<00:00, 10.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 2\t Loss: 1.084982\t Test Accuracy 71.770 \t lr: 0.0010\n",
      "Best accuracy: 71.770 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 600/600 [05:48<00:00,  1.72it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:09<00:00, 10.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 3\t Loss: 1.015165\t Test Accuracy 73.440 \t lr: 0.0010\n",
      "Best accuracy: 73.440 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 600/600 [05:59<00:00,  1.67it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:09<00:00, 10.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 4\t Loss: 0.972730\t Test Accuracy 74.250 \t lr: 0.0010\n",
      "Best accuracy: 74.250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 600/600 [05:44<00:00,  1.74it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:09<00:00, 10.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 5\t Loss: 0.945572\t Test Accuracy 75.370 \t lr: 0.0010\n",
      "Best accuracy: 75.370 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 600/600 [05:43<00:00,  1.75it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:09<00:00, 10.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 6\t Loss: 0.923917\t Test Accuracy 76.900 \t lr: 0.0010\n",
      "Best accuracy: 76.900 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 600/600 [05:54<00:00,  1.69it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:09<00:00, 10.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 7\t Loss: 0.902923\t Test Accuracy 77.060 \t lr: 0.0010\n",
      "Best accuracy: 77.060 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 600/600 [05:44<00:00,  1.74it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:09<00:00, 10.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 8\t Loss: 0.887622\t Test Accuracy 78.170 \t lr: 0.0010\n",
      "Best accuracy: 78.170 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 600/600 [05:51<00:00,  1.71it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:08<00:00, 11.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 9\t Loss: 0.876877\t Test Accuracy 77.490 \t lr: 0.0010\n",
      "Best accuracy: 78.170 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 600/600 [05:41<00:00,  1.76it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:09<00:00, 10.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 10\t Loss: 0.863229\t Test Accuracy 78.700 \t lr: 0.0010\n",
      "Best accuracy: 78.700 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 600/600 [05:46<00:00,  1.73it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:09<00:00, 11.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 11\t Loss: 0.851570\t Test Accuracy 79.330 \t lr: 0.0010\n",
      "Best accuracy: 79.330 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 600/600 [05:47<00:00,  1.73it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:09<00:00, 10.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 12\t Loss: 0.843434\t Test Accuracy 79.350 \t lr: 0.0010\n",
      "Best accuracy: 79.350 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 600/600 [05:46<00:00,  1.73it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:09<00:00, 11.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 13\t Loss: 0.837319\t Test Accuracy 80.100 \t lr: 0.0010\n",
      "Best accuracy: 80.100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 600/600 [05:47<00:00,  1.73it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:09<00:00, 10.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 14\t Loss: 0.828038\t Test Accuracy 80.350 \t lr: 0.0010\n",
      "Best accuracy: 80.350 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 600/600 [05:57<00:00,  1.68it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:09<00:00, 11.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 15\t Loss: 0.822933\t Test Accuracy 81.040 \t lr: 0.0010\n",
      "Best accuracy: 81.040 \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import importlib\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(\"./data\", download=True, transform=\n",
    "                                                transforms.Compose([transforms.ToTensor()]))\n",
    "test_set = torchvision.datasets.FashionMNIST(\"./data\", download=True, train=False, transform=\n",
    "                                               transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, \n",
    "                                           batch_size=100)\n",
    "test_loader = torch.utils.data.DataLoader(test_set,\n",
    "                                          batch_size=100)\n",
    "\n",
    "model = BNNCaffenet()\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "classification = BnnClassifier(model, train_loader, test_loader, device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "criterion.to(device)\n",
    "\n",
    "if hasattr(model, 'init_w'):\n",
    "    model.init_w()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr,momentum=0.9, weight_decay=1e-5)\n",
    "print(optimizer)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, steps, gamma=gamma)\n",
    "\n",
    "classification.train(criterion, optimizer, epochs, scheduler, checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fb56e8",
   "metadata": {},
   "source": [
    "In the future i plan to finish visaulisation of the result for presentation of the project and using the model with more number of epochs in order to find the optimal model settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24331ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
