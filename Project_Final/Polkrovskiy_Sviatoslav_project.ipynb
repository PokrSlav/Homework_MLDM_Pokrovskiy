{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63b3c40f",
   "metadata": {},
   "source": [
    "# Project Pokrovskiy Sviatoslav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a727746",
   "metadata": {},
   "source": [
    "## Paper: BinaryConnect: Training Deep Neural Networks with binary weights during propagations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8609b35d",
   "metadata": {},
   "source": [
    "The main point of the selected article that all my privious expirience using NN was finished at the moment of training then my computer could not make necessary calculations. I keen on studing varios way to using the deep leaning metods on not such power devices.\n",
    "In this part of project i try to understand the approach of BinaryConnect and use it in fasion MNIST dataset.\n",
    "The authors use library PyLearn2 wich doesn't work on python3 and higher. It was a great problem for me, but i use their code for understading using of BinaryConnect.\n",
    "Here i use pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a721b06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_label(label):\n",
    "    output_mapping = {\n",
    "                 0: \"T-shirt/Top\",\n",
    "                 1: \"Trouser\",\n",
    "                 2: \"Pullover\",\n",
    "                 3: \"Dress\",\n",
    "                 4: \"Coat\", \n",
    "                 5: \"Sandal\", \n",
    "                 6: \"Shirt\",\n",
    "                 7: \"Sneaker\",\n",
    "                 8: \"Bag\",\n",
    "                 9: \"Ankle Boot\"\n",
    "                 }\n",
    "    input = (label.item() if type(label) == torch.Tensor else label)\n",
    "    return output_mapping[input]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320ec3c9",
   "metadata": {},
   "source": [
    "### Customization the layers and conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70945c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Module, Conv2d, Linear\n",
    "from torch.nn.functional import linear, conv2d\n",
    "\n",
    "def Binarize(tensor,quant_mode='det'):\n",
    "    if quant_mode=='det':\n",
    "        return tensor.sign()\n",
    "    if quant_mode=='bin':\n",
    "        return (tensor>=0).type(type(tensor))*2-1\n",
    "    else:\n",
    "        return tensor.add_(1).div_(2).add_(torch.rand(tensor.size()).add(-0.5)).clamp_(0,1).round().mul_(2).add_(-1)\n",
    "\n",
    "\n",
    "class BNNLinear(Linear):\n",
    "\n",
    "    def __init__(self, *kargs, **kwargs):\n",
    "        super(BNNLinear, self).__init__(*kargs, **kwargs)\n",
    "        self.register_buffer('weight_org', self.weight.data.clone())\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        if (input.size(1) != 784) and (input.size(1) != 3072):\n",
    "            input.data=Binarize(input.data)\n",
    "            \n",
    "        self.weight.data=Binarize(self.weight_org)\n",
    "        out = linear(input, self.weight)\n",
    "\n",
    "        if not self.bias is None:\n",
    "            self.bias.org=self.bias.data.clone()\n",
    "            out += self.bias.view(1, -1).expand_as(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "class BNNConv2d(Conv2d):\n",
    "\n",
    "    def __init__(self, *kargs, **kwargs):\n",
    "        super(BNNConv2d, self).__init__(*kargs, **kwargs)\n",
    "        self.register_buffer('weight_org', self.weight.data.clone())\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.size(1) != 3:\n",
    "            input.data = Binarize(input.data)\n",
    "        \n",
    "        self.weight.data=Binarize(self.weight_org)\n",
    "        \n",
    "\n",
    "        out = conv2d(input, self.weight, None, self.stride,\n",
    "                                   self.padding, self.dilation, self.groups)\n",
    "\n",
    "        if not self.bias is None:\n",
    "            self.bias.org=self.bias.data.clone()\n",
    "            out += self.bias.view(1, -1, 1, 1).expand_as(out)\n",
    "\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f566a549",
   "metadata": {},
   "source": [
    "### Customization the model of BNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bf653cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BNNCaffenet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(BNNCaffenet, self).__init__()\n",
    " \n",
    "        self.features = nn.Sequential(\n",
    "                \n",
    "                BNNConv2d(1, 96, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(96),\n",
    "                nn.Hardtanh(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=True),\n",
    "                \n",
    "                BNNConv2d(96, 192, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(192),\n",
    "                nn.Hardtanh(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=True),\n",
    "                \n",
    "                BNNConv2d(192, 288, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(288),\n",
    "                nn.Hardtanh(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=True),\n",
    "                \n",
    "                nn.Flatten(),\n",
    "                nn.BatchNorm1d(4608),\n",
    "                nn.Hardtanh(inplace=True),\n",
    "                BNNLinear(4608, num_classes),\n",
    "                nn.BatchNorm1d(num_classes, affine=False),\n",
    "                nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "\n",
    "\n",
    "    def init_w(self):\n",
    "        # weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "        return\n",
    "\n",
    "\n",
    "def bnn_caffenet(num_classes=10):\n",
    "    return BNNCaffenet(num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fc7b87",
   "metadata": {},
   "source": [
    "### Customization the classifier of the model and saving the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62ea8786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torch import save, no_grad\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "\n",
    "class Classifier():\n",
    "    def __init__(self, model, train_loader=None, test_loader=None, device=None):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def save_checkpoint(state, is_best, checkpoint):\n",
    "        head, tail = os.path.split(checkpoint)\n",
    "        if not os.path.exists(head):\n",
    "            os.makedirs(head)\n",
    "\n",
    "        filename = os.path.join(head, '{0}_checkpoint.pth.tar'.format(tail))\n",
    "        save(state, filename)\n",
    "        if is_best:\n",
    "            shutil.copyfile(filename, os.path.join(head,\n",
    "                '{0}_best.pth.tar'.format(tail)))\n",
    "\n",
    "        return\n",
    "\n",
    "    def test(self, criterion):\n",
    "        self.model.eval()\n",
    "        top1 = 0\n",
    "        test_loss = 0.\n",
    "\n",
    "        with no_grad():\n",
    "            for data, target in tqdm(self.test_loader):\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                output = self.model(data)\n",
    "                test_loss += criterion(output, target).item()\n",
    "                pred = output.argmax(dim=1, keepdim=True)      \n",
    "                top1 += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        top1_acc = 100. * top1 / len(self.test_loader.sampler)\n",
    "\n",
    "        return top1_acc\n",
    "\n",
    "\n",
    "    def top1_accuracy(self):\n",
    "        return top1_accuracy(self.model, self.test_loader, self.device)\n",
    "\n",
    "\n",
    "    \n",
    "    def train_step(self, criterion, optimizer):\n",
    "        losses = []\n",
    "        for data, target in tqdm(self.train_loader,\n",
    "                total=len(self.train_loader)):\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            output = self.model(data)\n",
    "            loss = criterion(output, target)\n",
    "            losses.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            for p in self.model.modules():\n",
    "                if hasattr(p, 'weight_org'):\n",
    "                    p.weight.data.copy_(p.weight_org)\n",
    "            optimizer.step()\n",
    "            for p in self.model.modules():\n",
    "                if hasattr(p, 'weight_org'):\n",
    "                    p.weight_org.data.copy_(p.weight.data.clamp_(-1,1))\n",
    "        return losses\n",
    "  \n",
    "    \n",
    "    def train(self, criterion, optimizer, epochs, scheduler,\n",
    "            checkpoint=None):\n",
    "\n",
    "        if checkpoint is None:\n",
    "            raise ValueError('Specify a valid checkpoint')\n",
    "\n",
    "        \n",
    "        best_accuracy = 0.\n",
    "\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "\n",
    "        for epoch in range(1, epochs+1):\n",
    "            self.model.train()\n",
    "            epoch_losses = self.train_step(criterion, optimizer)\n",
    "            losses += epoch_losses\n",
    "            epoch_losses = np.array(epoch_losses)\n",
    "            lr = optimizer.param_groups[0]['lr']  \n",
    "            test_accuracy = self.test(criterion)\n",
    "            accuracies.append(test_accuracy)\n",
    "            if scheduler:     \n",
    "                scheduler.step()\n",
    "            is_best = test_accuracy > best_accuracy\n",
    "            if is_best:\n",
    "                best_accuracy = test_accuracy\n",
    "            \n",
    "            print('Train Epoch {0}\\t Loss: {1:.6f}\\t Test Accuracy {2:.3f} \\t lr: {3:.4f}'\n",
    "                    .format(epoch, epoch_losses.mean(), test_accuracy, lr))\n",
    "            print('Best accuracy: {:.3f} '.format(best_accuracy))\n",
    "\n",
    "            self.save_checkpoint({\n",
    "                'epoch': epoch+1,\n",
    "                'state_dict': self.model.state_dict(),\n",
    "                'best_accuracy': best_accuracy,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'criterion': criterion,\n",
    "                }, is_best, checkpoint)\n",
    "\n",
    "        plt.plot(accuracies)\n",
    "        plt.xlabel(\"No. of Epoch\")\n",
    "        plt.ylabel(\"Accuracies\")\n",
    "        plt.title(\"Accuracies by epoch\")\n",
    "        plt.show()\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f7dbe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes = 10):\n",
    "        super(FashionNN, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 96, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.Hardtanh(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=True),\n",
    "            \n",
    "            nn.Conv2d(96, 192, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.Hardtanh(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=True),\n",
    "            \n",
    "            nn.Conv2d(192, 288, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(288),\n",
    "            nn.Hardtanh(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=True),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.BatchNorm1d(4608),\n",
    "            nn.Hardtanh(inplace=True),\n",
    "            nn.Linear(4608, num_classes),\n",
    "            nn.BatchNorm1d(num_classes, affine=False),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f2b51d",
   "metadata": {},
   "source": [
    "### Configurable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98809794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR = 0.001\n",
      "Steps = [80, 200]\n",
      "Gamma = 0.1\n",
      "num_epochs = 15\n",
      "Checkpoint = results/bnn_caffenet_fasion_MNIST\n"
     ]
    }
   ],
   "source": [
    "# LR \n",
    "lr = .001\n",
    "print(\"LR = \"+str(lr))\n",
    "\n",
    "# Steps\n",
    "steps = [80,200]\n",
    "print(\"Steps = \"+str(steps))\n",
    "\n",
    "# Gamma\n",
    "gamma = 0.1\n",
    "print(\"Gamma = \"+str(gamma))\n",
    "\n",
    "# Num_epochs\n",
    "epochs = 15\n",
    "print(\"num_epochs = \"+str(epochs))\n",
    "\n",
    "# Checkpoint\n",
    "checkpoint = \"results/bnn_caffenet_fasion_MNIST\"\n",
    "print(\"Checkpoint = \"+str(checkpoint))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbef600",
   "metadata": {},
   "source": [
    "### Realization of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3c4572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_label(label):\n",
    "    output_mapping = {\n",
    "                 0: \"T-shirt/Top\",\n",
    "                 1: \"Trouser\",\n",
    "                 2: \"Pullover\",\n",
    "                 3: \"Dress\",\n",
    "                 4: \"Coat\", \n",
    "                 5: \"Sandal\", \n",
    "                 6: \"Shirt\",\n",
    "                 7: \"Sneaker\",\n",
    "                 8: \"Bag\",\n",
    "                 9: \"Ankle Boot\"\n",
    "                 }\n",
    "    input = (label.item() if type(label) == torch.Tensor else label)\n",
    "    return output_mapping[input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6c95988",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▊                                        | 26/600 [00:11<04:17,  2.23it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yn/k_xt5k715x5bwhj3pt15z5200000gn/T/ipykernel_3773/1187428697.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mclassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/yn/k_xt5k715x5bwhj3pt15z5200000gn/T/ipykernel_3773/3640614492.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, criterion, optimizer, epochs, scheduler, checkpoint)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mepoch_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mepoch_losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mepoch_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/yn/k_xt5k715x5bwhj3pt15z5200000gn/T/ipykernel_3773/3640614492.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, criterion, optimizer)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weight_org'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(\"./data\", download=True, transform=\n",
    "                                                transforms.Compose([transforms.ToTensor()]))\n",
    "test_set = torchvision.datasets.FashionMNIST(\"./data\", download=True, train=False, transform=\n",
    "                                               transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, \n",
    "                                           batch_size=100)\n",
    "test_loader = torch.utils.data.DataLoader(test_set,\n",
    "                                          batch_size=100)\n",
    "\n",
    "model = BNNCaffenet()\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "classification = Classifier(model, train_loader, test_loader, device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "criterion.to(device)\n",
    "\n",
    "if hasattr(model, 'init_w'):\n",
    "    model.init_w()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr,momentum=0.9, weight_decay=1e-5)\n",
    "#print(optimizer)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, steps, gamma=gamma)\n",
    "\n",
    "classification.train(criterion, optimizer, epochs, scheduler, checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d151b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = [0. for _ in range(10)]\n",
    "total_correct = [0. for _ in range(10)]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        test = Variable(images)\n",
    "        outputs = model(test)\n",
    "        predicted = torch.max(outputs, 1)[1]\n",
    "        c = (predicted == labels).squeeze()\n",
    "        \n",
    "        for i in range(100):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            total_correct[label] += 1\n",
    "        \n",
    "for i in range(10):\n",
    "    print(\"Accuracy of {}: {:.2f}%\".format(output_label(i), class_correct[i] * 100 / total_correct[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fb56e8",
   "metadata": {},
   "source": [
    "In the future i plan to finish visaulisation of the result for presentation of the project and using the model with more number of epochs in order to find the optimal model settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08872814",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 600/600 [04:21<00:00,  2.30it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:08<00:00, 11.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 1\t Loss: 0.707680\t Test Accuracy 87.640 \t lr: 0.0010\n",
      "Best accuracy: 87.640 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████████████████████                    | 308/600 [02:14<02:08,  2.28it/s]"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(\"./data\", download=True, transform=\n",
    "                                                transforms.Compose([transforms.ToTensor()]))\n",
    "test_set = torchvision.datasets.FashionMNIST(\"./data\", download=True, train=False, transform=\n",
    "                                               transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, \n",
    "                                           batch_size=100)\n",
    "test_loader = torch.utils.data.DataLoader(test_set,\n",
    "                                          batch_size=100)\n",
    "\n",
    "model_1 = FashionNN()\n",
    "model_1.to(device)\n",
    "\n",
    "\n",
    "classification = Classifier(model_1, train_loader, test_loader, device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "criterion.to(device)\n",
    "\n",
    "#if hasattr(model_1, 'init_w'):\n",
    "#    model.init_w()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(model_1.parameters(), lr=lr,momentum=0.9, weight_decay=1e-5)\n",
    "#print(optimizer)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, steps, gamma=gamma)\n",
    "\n",
    "classification.train(criterion, optimizer, epochs, scheduler, checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f91725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = [0. for _ in range(10)]\n",
    "total_correct = [0. for _ in range(10)]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        test = Variable(images)\n",
    "        outputs = model(test)\n",
    "        predicted = torch.max(outputs, 1)[1]\n",
    "        c = (predicted == labels).squeeze()\n",
    "        \n",
    "        for i in range(100):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            total_correct[label] += 1\n",
    "        \n",
    "for i in range(10):\n",
    "    print(\"Accuracy of {}: {:.2f}%\".format(output_label(i), class_correct[i] * 100 / total_correct[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009f28fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
